# Traditional Classification Models (Scikit-Learn & XGBoost)
LogisticRegression:
  penalty: ['l1', 'l2']
  C: [0.01, 0.1, 1, 10]
  'solver': ['liblinear', 'saga']

DecisionTreeClassifier:
  criterion: ['gini', 'entropy']
  max_depth: [5, 10, 20]
  min_samples_split: [2, 5, 10]
  min_samples_leaf: [1, 2, 5]

RandomForestClassifier:
  n_estimators: [50, 100, 200, 500]
  max_depth: [10, 20, 30, None]
  min_samples_split: [2, 5, 10]
  min_samples_leaf: [1, 2, 5]
  bootstrap: [True, False]

GradientBoostingClassifier:
  learning_rate: [0.01, 0.1, 0.2, 0.3]
  n_estimators: [100, 200, 500]
  subsample: [0.6, 0.8, 1.0]
  max_depth: [3, 5, 10]
  min_samples_split: [2, 5, 10]

XGBClassifier:
  learning_rate: [0.01, 0.1, 0.2, 0.3]
  n_estimators: [100, 200, 500]
  max_depth: [3, 5, 10]
  min_child_weight: [1, 3, 5]
  gamma: [0, 0.1, 0.2]
  subsample: [0.6, 0.8, 1.0]
  colsample_bytree: [0.6, 0.8, 1.0]

SVC:
  kernel: ['linear', 'poly', 'rbf', 'sigmoid']
  C: [0.1, 1, 10, 100]
  gamma: ['scale', 'auto']
  probability: [True, False]

KNeighborsClassifier:
  n_neighbors: [3, 5, 10, 15]
  weights: ['uniform', 'distance']
  metric: ['euclidean', 'manhattan', 'minkowski']

MLPClassifier:
  hidden_layer_sizes: [(50,), (100,), (150,)]
  activation: ['relu', 'tanh']
  alpha: [0.0001, 0.001, 0.01]
  learning_rate: ['constant', 'invscaling', 'adaptive']
  max_iter: [200, 500, 1000]

# Deep Learning Classification Models (Keras)
DeepLearningClassifier:
  num_layers: [2, 3, 4]  # Number of hidden layers
  units_per_layer: [64, 128, 256]  # Number of neurons in each hidden layer
  activation: ['relu', 'tanh', 'sigmoid']
  dropout_rate: [0.0, 0.2, 0.5]
  optimizer: ['adam', 'sgd', 'rmsprop']
  learning_rate: [0.001, 0.01, 0.1]
  batch_size: [16, 32, 64]
  epochs: [50, 100, 200]
  loss_function: ['categorical_crossentropy', 'sparse_categorical_crossentropy', 'binary_crossentropy']
